{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "797be6a9",
   "metadata": {},
   "source": [
    "# HealthBot: Experiment 1 - Hyperparameter Configuration\n",
    "## Fine-tuning TinyLlama with LoRA on a Health-Centric Dataset\n",
    "\n",
    "**Domain:** Healthcare / Medical Q&A  \n",
    "**Model:** TinyLlama-1.1B-Chat  \n",
    "**Technique:** Parameter-Efficient Fine-Tuning (PEFT) with LoRA  \n",
    "**Dataset:** MedQuAD (Medical Question Answering Dataset)\n",
    "\n",
    "---\n",
    "### Experiment 1 Configuration\n",
    "**Learning Rate:** 2e-4  \n",
    "**Batch Size:** 2  \n",
    "**Gradient Accumulation Steps:** 4  \n",
    "**Epochs:** 1  \n",
    "**LoRA Rank:** 8  \n",
    "**Expected Train Loss:** 1.82  \n",
    "**Expected Val Loss:** 1.91  \n",
    "**Expected GPU Memory:** 9.2 GB  \n",
    "**Expected Training Time:** ~18 minutes\n",
    "\n",
    "---\n",
    "### Project Overview\n",
    "This notebook fine-tunes TinyLlama on a curated subset of the MedQuAD dataset to build a domain-specific health assistant capable of answering common medical questions. This is Experiment 1 with specific hyperparameters to test faster training with lower LoRA rank.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d68893e",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPLETE PACKAGE INSTALLATION - Tested Compatible Versions\n",
    "# ============================================================================\n",
    "# This cell installs all required packages with specific versions that are\n",
    "# known to work together. After running this cell, RESTART YOUR RUNTIME!\n",
    "# (Runtime → Restart runtime in Colab)\n",
    "# ============================================================================\n",
    "\n",
    "!pip install -q transformers datasets peft accelerate bitsandbytes gradio evaluate rouge_score bert_score nltk sentencepiece torch\n",
    "\n",
    "# Install required libraries\n",
    "\n",
    "print(' All packages installed successfully!')\n",
    "print(' CRITICAL: Restart your runtime now!')\n",
    "print(' In Colab: Runtime → Restart runtime')\n",
    "print(' Then continue from Cell 4 (skip this installation cell)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7999a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "\n",
    "# HuggingFace & PEFT imports\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    GenerationConfig\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "# from trl import SFTTrainer\n",
    "\n",
    "# Check GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'  Device: {device}')\n",
    "if device == 'cuda':\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'   GPU: {gpu_name}')\n",
    "    print(f'   VRAM: {gpu_mem:.1f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc544d",
   "metadata": {},
   "source": [
    "## 2. Dataset Collection & Preprocessing\n",
    "\n",
    "We use the **MedQuAD** dataset (Medical Question Answering Dataset) from Hugging Face Datasets Hub. It contains 47,457 medical Q&A pairs sourced from 12 NIH websites, covering diseases, drugs, symptoms, and treatments.\n",
    "\n",
    "**Source:** `lavita/MedQuAD` on Hugging Face Datasets  \n",
    "**Size Used:** ~3,000 high-quality QA pairs (subset for efficient training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8b328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2.1  Load the MedQuAD dataset\n",
    "# ─────────────────────────────────────────────\n",
    "print(' Loading MedQuAD dataset from Hugging Face...')\n",
    "raw_dataset = load_dataset('lavita/MedQuAD', trust_remote_code=True)\n",
    "print(raw_dataset)\n",
    "print(f'\\nSample entry:\\n{raw_dataset[\"train\"][0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas==2.2.2 pyarrow==15.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9ae747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2.2  Exploratory Data Analysis\n",
    "# ─────────────────────────────────────────────\n",
    "df = raw_dataset['train'].to_pandas()\n",
    "print('Dataset shape:', df.shape)\n",
    "print('\\nColumn dtypes:')\n",
    "print(df.dtypes)\n",
    "print('\\nNull counts:')\n",
    "print(df.isnull().sum())\n",
    "print('\\nFirst 3 rows:')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28bb82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2.3  Data Cleaning & Preprocessing\n",
    "# ─────────────────────────────────────────────\n",
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Normalize and clean raw text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # Collapse multiple whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Identify question and answer columns\n",
    "# MedQuAD uses 'question' and 'answer'\n",
    "Q_COL = 'question'\n",
    "A_COL = 'answer'\n",
    "\n",
    "# Clean columns\n",
    "df['question_clean'] = df[Q_COL].apply(clean_text)\n",
    "df['answer_clean']   = df[A_COL].apply(clean_text)\n",
    "\n",
    "# Remove rows with missing/empty question or answer\n",
    "df = df[(df['question_clean'].str.len() > 10) &\n",
    "        (df['answer_clean'].str.len()   > 20)].copy()\n",
    "\n",
    "# Filter out very long answers that would exceed context window\n",
    "# Keep answers under 512 words for efficient training\n",
    "df['answer_word_count'] = df['answer_clean'].apply(lambda x: len(x.split()))\n",
    "df = df[df['answer_word_count'] <= 300].copy()\n",
    "\n",
    "print(f'Cleaned dataset size: {len(df)}')\n",
    "print(f'Average question length (words): {df[\"question_clean\"].apply(lambda x: len(x.split())).mean():.1f}')\n",
    "print(f'Average answer length (words): {df[\"answer_word_count\"].mean():.1f}')\n",
    "\n",
    "# Plot answer length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "df['question_clean'].apply(lambda x: len(x.split())).hist(bins=30, ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Question Length Distribution (words)')\n",
    "axes[0].set_xlabel('Word Count')\n",
    "df['answer_word_count'].hist(bins=30, ax=axes[1], color='coral')\n",
    "axes[1].set_title('Answer Length Distribution (words)')\n",
    "axes[1].set_xlabel('Word Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig('length_distribution_exp1.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(' EDA complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0191fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2.4  Format into Instruction-Response Template\n",
    "# ─────────────────────────────────────────────\n",
    "# We use a standard ChatML / instruction-tuning template.\n",
    "# This is the format TinyLlama-Chat was pre-trained on.\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are HealthBot, a knowledgeable and empathetic medical information assistant. \"\n",
    "    \"You provide accurate, evidence-based health information to help users understand \"\n",
    "    \"medical conditions, symptoms, and treatments. Always remind users to consult a \"\n",
    "    \"qualified healthcare professional for personal medical advice.\"\n",
    ")\n",
    "\n",
    "def format_instruction(question: str, answer: str) -> str:\n",
    "    \"\"\"Format a QA pair into TinyLlama ChatML instruction format.\"\"\"\n",
    "    return (\n",
    "        f\"<|system|>\\n{SYSTEM_PROMPT}</s>\\n\"\n",
    "        f\"<|user|>\\n{question}</s>\\n\"\n",
    "        f\"<|assistant|>\\n{answer}</s>\"\n",
    "    )\n",
    "\n",
    "# Sample to target 1,500 diverse examples (reduced for faster training)\n",
    "TARGET_N = 1500\n",
    "df_sample = df.sample(n=min(TARGET_N, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_sample['text'] = df_sample.apply(\n",
    "    lambda row: format_instruction(row['question_clean'], row['answer_clean']), axis=1\n",
    ")\n",
    "\n",
    "print(f'Total formatted examples: {len(df_sample)}')\n",
    "print('\\n--- Sample Formatted Entry ---')\n",
    "print(df_sample['text'].iloc[0][:800], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa864d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \\\n",
    "datasets==2.19.1 \\\n",
    "pyarrow==15.0.2 \\\n",
    "pandas==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8040f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 2.5  Train / Validation / Test Split\n",
    "# ─────────────────────────────────────────────\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(df_sample, test_size=0.2, random_state=42)\n",
    "val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df[['text']].reset_index(drop=True))\n",
    "val_dataset   = Dataset.from_pandas(val_df[['text']].reset_index(drop=True))\n",
    "test_dataset  = Dataset.from_pandas(test_df[['text']].reset_index(drop=True))\n",
    "\n",
    "print(f'Train size : {len(train_dataset)}')\n",
    "print(f'Val size   : {len(val_dataset)}')\n",
    "print(f'Test size  : {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea98a6",
   "metadata": {},
   "source": [
    "## 3. Model Loading & LoRA Configuration\n",
    "\n",
    "We use **TinyLlama-1.1B-Chat-v1.0** — a compact yet capable LLM well-suited for Colab's free T4 GPU.  \n",
    "**LoRA (Low-Rank Adaptation)** freezes base model weights and injects small trainable rank-decomposition matrices, reducing trainable parameters by ~99% while preserving model quality.\n",
    "\n",
    "**Experiment 1 Configuration:** LoRA rank = 8 (lower rank for faster training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4838725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 3.1  Load Tokenizer\n",
    "# ─────────────────────────────────────────────\n",
    "MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token    = tokenizer.eos_token   # TinyLlama has no pad token by default\n",
    "tokenizer.padding_side = 'right'               # Right-pad for causal LM training\n",
    "\n",
    "print(f'Vocab size      : {tokenizer.vocab_size}')\n",
    "print(f'Model max length: {tokenizer.model_max_length}')\n",
    "print(f'Pad token       : \"{tokenizer.pad_token}\" (id={tokenizer.pad_token_id})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes>=0.46.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c77b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 3.2  Quantization Config (4-bit QLoRA)\n",
    "# ─────────────────────────────────────────────\n",
    "# QLoRA loads the base model in 4-bit precision to save VRAM,\n",
    "# then applies LoRA adapters in float16 for stable training.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit               = True,\n",
    "    bnb_4bit_use_double_quant  = True,         # double quantization for extra compression\n",
    "    bnb_4bit_quant_type        = 'nf4',        # NormalFloat4 — best for LLMs\n",
    "    bnb_4bit_compute_dtype     = torch.float16  # Use float16 for broader GPU compatibility\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 3.3  Load Base Model (4-bit)\n",
    "# ─────────────────────────────────────────────\n",
    "print(f' Loading {MODEL_NAME} in 4-bit...')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map          = 'auto',\n",
    "    trust_remote_code   = True\n",
    ")\n",
    "base_model.config.use_cache = False              # required for gradient checkpointing\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# Count base model parameters\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(f' Model loaded  | Total parameters: {total_params/1e9:.2f}B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc6839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 3.4  LoRA Configuration - EXPERIMENT 1\n",
    "# ─────────────────────────────────────────────\n",
    "# LoRA injects trainable rank-r matrices into attention projections.\n",
    "# EXPERIMENT 1: rank = 8 (lower rank for faster training)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type      = TaskType.CAUSAL_LM,\n",
    "    r              = 8,           # EXPERIMENT 1: Lower rank\n",
    "    lora_alpha     = 16,          # Scaling factor (alpha/r = 2 recommended)\n",
    "    lora_dropout   = 0.05,        # Dropout on LoRA layers\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj',\n",
    "                      'gate_proj', 'up_proj', 'down_proj'],  # Target all linear layers\n",
    "    bias           = 'none',\n",
    ")\n",
    "\n",
    "# Wrap model with PEFT\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Print trainable parameter summary\n",
    "trainable     = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total         = sum(p.numel() for p in model.parameters())\n",
    "print(f'Trainable params : {trainable/1e6:.2f}M  ({100*trainable/total:.2f}% of total)')\n",
    "print(f'Total params     : {total/1e9:.2f}B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1888a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/ML-Techniques-Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a46f8",
   "metadata": {},
   "source": [
    "## 4. Model Fine-Tuning - Experiment 1 Configuration\n",
    "\n",
    "**Experiment 1 Hyperparameters:**\n",
    "- Learning Rate: 2e-4\n",
    "- Batch Size: 2\n",
    "- Gradient Accumulation: 4\n",
    "- Epochs: 1\n",
    "- LoRA Rank: 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14cc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "OUTPUT_DIR = '/content/drive/MyDrive/ML-Techniques-Fine-tuning/healthbot_tinyllama_lora_exp1'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                  = OUTPUT_DIR,\n",
    "    num_train_epochs            = 1,             # EXPERIMENT 1: 1 epoch\n",
    "    per_device_train_batch_size = 2,             # EXPERIMENT 1: batch size 2\n",
    "    per_device_eval_batch_size  = 2,\n",
    "    gradient_accumulation_steps = 4,             # EXPERIMENT 1: grad accum 4\n",
    "    learning_rate               = 2e-4,          # EXPERIMENT 1: LR 2e-4\n",
    "    lr_scheduler_type           = 'cosine',\n",
    "    warmup_ratio                = 0.05,\n",
    "    weight_decay                = 0.001,\n",
    "    optim                       = 'paged_adamw_8bit',\n",
    "    fp16                        = False,\n",
    "    bf16                        = False,\n",
    "    max_grad_norm               = 0.3,\n",
    "    gradient_checkpointing      = True,\n",
    "    logging_steps               = 25,\n",
    "    eval_strategy               = 'steps',\n",
    "    eval_steps                  = 100,\n",
    "    save_strategy               = 'steps',\n",
    "    save_steps                  = 200,\n",
    "    load_best_model_at_end      = True,\n",
    "    metric_for_best_model       = 'loss',\n",
    "    report_to                   = 'none',\n",
    "    push_to_hub                 = False,\n",
    ")\n",
    "\n",
    "print(' Training arguments configured for Experiment 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff3f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 4.2  SFT Trainer Setup\n",
    "# ─────────────────────────────────────────────\n",
    "from trl import SFTTrainer\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Clear CUDA cache before training to free up memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\" CUDA cache cleared\")\n",
    "\n",
    "# Enable gradient checkpointing on the model\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Create a tokenization function that limits sequence length\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize and truncate to 512 tokens to save memory.\"\"\"\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=False, remove_columns=[\"text\"])\n",
    "tokenized_val = val_dataset.map(tokenize_function, batched=False, remove_columns=[\"text\"])\n",
    "\n",
    "# Data collator for dynamic padding\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\" Trainer initialized with Experiment 1 hyperparameters\")\n",
    "\n",
    "# Log initial GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "    reserved  = torch.cuda.memory_reserved(0)  / 1e9\n",
    "    print(f'   GPU Memory — Allocated: {allocated:.2f} GB | Reserved: {reserved:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f12d383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 4.3  Train the Model\n",
    "# ─────────────────────────────────────────────\n",
    "# Clear CUDA cache one more time before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\" Final CUDA cache clear before training\")\n",
    "\n",
    "print(' Starting fine-tuning (Experiment 1)...')\n",
    "start_time = time.time()\n",
    "\n",
    "train_result = trainer.train()\n",
    "\n",
    "elapsed = (time.time() - start_time) / 60\n",
    "print(f'\\n Training complete in {elapsed:.1f} minutes')\n",
    "print(f'   Train loss : {train_result.training_loss:.4f}')\n",
    "\n",
    "# Log final GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    peak_mem = torch.cuda.max_memory_allocated(0) / 1e9\n",
    "    print(f' Peak GPU mem: {peak_mem:.2f} GB')\n",
    "\n",
    "# Save LoRA adapter weights\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f' Adapter saved to {OUTPUT_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 4.4  Plot Training & Validation Loss Curves\n",
    "# ─────────────────────────────────────────────\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "train_steps  = [e['step'] for e in log_history if 'loss' in e and 'eval_loss' not in e]\n",
    "train_losses = [e['loss'] for e in log_history if 'loss' in e and 'eval_loss' not in e]\n",
    "val_steps    = [e['step'] for e in log_history if 'eval_loss' in e]\n",
    "val_losses   = [e['eval_loss'] for e in log_history if 'eval_loss' in e]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_steps, train_losses, label='Train Loss', color='steelblue', linewidth=2)\n",
    "plt.plot(val_steps,   val_losses,   label='Val Loss',   color='coral',     linewidth=2, linestyle='--')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss — HealthBot Experiment 1')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_curve_exp1.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d506804",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "We evaluate with multiple metrics:\n",
    "- **ROUGE-1 / ROUGE-2 / ROUGE-L** — measures n-gram overlap between generated and reference answers\n",
    "- **BLEU Score** — precision-based n-gram overlap metric\n",
    "- **Perplexity** — model confidence on held-out test set\n",
    "- **Qualitative Testing** — hand-crafted prompts covering in-domain and out-of-domain queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f45f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 5.1  Load Evaluation Metrics\n",
    "# ─────────────────────────────────────────────\n",
    "rouge  = evaluate.load('rouge')\n",
    "bleu   = evaluate.load('bleu')\n",
    "\n",
    "print(' Metrics loaded: ROUGE, BLEU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89598295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 5.2  Inference Helper\n",
    "# ─────────────────────────────────────────────\n",
    "def generate_response(question: str,\n",
    "                      mdl,\n",
    "                      tok,\n",
    "                      max_new_tokens: int = 256,\n",
    "                      temperature: float = 0.3) -> str:\n",
    "    \"\"\"Generate a response from the model for a given health question.\"\"\"\n",
    "    prompt = (\n",
    "        f\"<|system|>\\n{SYSTEM_PROMPT}</s>\\n\"\n",
    "        f\"<|user|>\\n{question}</s>\\n\"\n",
    "        f\"<|assistant|>\\n\"\n",
    "    )\n",
    "    inputs = tok(prompt, return_tensors='pt').to(mdl.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = mdl.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens    = max_new_tokens,\n",
    "            temperature       = temperature,\n",
    "            do_sample         = True,\n",
    "            top_p             = 0.9,\n",
    "            repetition_penalty= 1.15,\n",
    "            pad_token_id      = tok.eos_token_id\n",
    "        )\n",
    "    # Decode only newly generated tokens\n",
    "    gen_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    return tok.decode(gen_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "print(' Inference helper ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc2233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# ─────────────────────────────────────────────\n",
    "# 5.3  Compute ROUGE & BLEU on Test Set\n",
    "# ─────────────────────────────────────────────\n",
    "# Extract questions and answers from test_dataset\n",
    "eval_size = min(20, len(test_dataset))\n",
    "eval_texts = [test_dataset[i]['text'] for i in range(eval_size)]\n",
    "\n",
    "# Parse out question and answer from formatted text\n",
    "def parse_qa(formatted_text):\n",
    "    \"\"\"Extract question and answer from formatted instruction text.\"\"\"\n",
    "    try:\n",
    "        # Extract question between <|user|> and </s>\n",
    "        q_start = formatted_text.find('<|user|>\\n') + 9\n",
    "        q_end = formatted_text.find('</s>', q_start)\n",
    "        question = formatted_text[q_start:q_end].strip()\n",
    "\n",
    "        # Extract answer between <|assistant|> and </s>\n",
    "        a_start = formatted_text.find('<|assistant|>\\n') + 14\n",
    "        a_end = formatted_text.find('</s>', a_start)\n",
    "        answer = formatted_text[a_start:a_end].strip()\n",
    "\n",
    "        return question, answer\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "eval_questions = []\n",
    "eval_references = []\n",
    "for text in eval_texts:\n",
    "    q, a = parse_qa(text)\n",
    "    if q and a:\n",
    "        eval_questions.append(q)\n",
    "        eval_references.append(a)\n",
    "\n",
    "print(f'Prepared {len(eval_questions)} test samples for evaluation')\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenizer, questions, references, model_name):\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    predictions_text = []\n",
    "    references_text = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for q, ref in zip(questions, references):\n",
    "        inputs = tokenizer(\n",
    "            q,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=150,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        # CRITICAL FIX: decode to STRING\n",
    "        pred_text = tokenizer.decode(\n",
    "            outputs[0],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        # Ensure reference is also STRING\n",
    "        if isinstance(ref, list):\n",
    "            ref = \" \".join(ref)\n",
    "\n",
    "        predictions_text.append(pred_text)\n",
    "        references_text.append(ref.strip())\n",
    "\n",
    "    # ── Metrics expect STRINGS ──\n",
    "    bleu_score = bleu.compute(\n",
    "        predictions=predictions_text,\n",
    "        references=[[r] for r in references_text]\n",
    "    )\n",
    "\n",
    "    rouge_score = rouge.compute(\n",
    "        predictions=predictions_text,\n",
    "        references=references_text\n",
    "    )\n",
    "\n",
    "    print(f\"\\n Evaluation Results — {model_name}\")\n",
    "    print(f\"BLEU:   {bleu_score['bleu']:.4f}\")\n",
    "    print(f\"ROUGE: {rouge_score}\")\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu_score,\n",
    "        \"rouge\": rouge_score,\n",
    "        \"predictions\": predictions_text,\n",
    "        \"references\": references_text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b51b612",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_results = evaluate_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    eval_questions,\n",
    "    eval_references,\n",
    "    \"Experiment 1 - HealthBot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13ea2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 5.4  Perplexity on Test Set\n",
    "# ─────────────────────────────────────────────\n",
    "def compute_perplexity(mdl, tok, texts, max_len=512):\n",
    "    \"\"\"Compute average perplexity over a list of text strings.\"\"\"\n",
    "    mdl.eval()\n",
    "    total_loss = 0.0\n",
    "    count      = 0\n",
    "    for text in texts:\n",
    "        enc = tok(text, return_tensors='pt',\n",
    "                  truncation=True, max_length=max_len).to(mdl.device)\n",
    "        with torch.no_grad():\n",
    "            output = mdl(**enc, labels=enc['input_ids'])\n",
    "        total_loss += output.loss.item()\n",
    "        count += 1\n",
    "    avg_loss = total_loss / count\n",
    "    return float(np.exp(avg_loss))\n",
    "\n",
    "test_texts = test_df['text'].head(50).tolist()\n",
    "ppl_ft     = compute_perplexity(model, tokenizer, test_texts)\n",
    "print(f'\\n Perplexity (Experiment 1): {ppl_ft:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 5.5  Metrics Visualization\n",
    "# ─────────────────────────────────────────────\n",
    "metrics = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU']\n",
    "\n",
    "ft_vals = [\n",
    "    ft_results['rouge']['rouge1'],\n",
    "    ft_results['rouge']['rouge2'],\n",
    "    ft_results['rouge']['rougeL'],\n",
    "    ft_results['bleu']['bleu']\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(metrics, ft_vals, color='steelblue', width=0.6)\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Experiment 1 — NLP Metrics')\n",
    "ax.set_ylim(0, max(ft_vals) * 1.2)\n",
    "\n",
    "# Annotate bars\n",
    "for bar, val in zip(bars, ft_vals):\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., h + 0.01, f'{val:.3f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_exp1.png', dpi=120, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('\\n Evaluation Metrics (Experiment 1):')\n",
    "for m, val in zip(metrics, ft_vals):\n",
    "    print(f'  {m:10s}: {val:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2fba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────\n",
    "# 5.6  Qualitative Testing\n",
    "# ─────────────────────────────────────────────\n",
    "test_queries = [\n",
    "    # In-domain (health)\n",
    "    ('What are the symptoms of Type 2 diabetes?', 'in-domain'),\n",
    "    ('How does hypertension affect the heart?',   'in-domain'),\n",
    "    ('What is the recommended treatment for asthma?', 'in-domain'),\n",
    "    ('Can you explain what a BMI of 28 means?',   'in-domain'),\n",
    "    # Out-of-domain\n",
    "    ('What is the capital of France?',            'out-of-domain'),\n",
    "    ('Help me write a Python function.',          'out-of-domain'),\n",
    "]\n",
    "\n",
    "print('='*70)\n",
    "print('QUALITATIVE EVALUATION — Experiment 1 HealthBot Responses')\n",
    "print('='*70)\n",
    "\n",
    "for question, domain_type in test_queries:\n",
    "    response = generate_response(question, model, tokenizer)\n",
    "    print(f'\\n[{domain_type.upper()}]')\n",
    "    print(f'Q: {question}')\n",
    "    print(f'A: {response}')\n",
    "    print('-'*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a978c6d",
   "metadata": {},
   "source": [
    "## 6. Summary & Conclusions - Experiment 1\n",
    "\n",
    "### Experiment 1 Configuration Summary\n",
    "\n",
    "| Component | Details |\n",
    "|-----------|----------|\n",
    "| **Learning Rate** | 2e-4 |\n",
    "| **Batch Size** | 2 |\n",
    "| **Gradient Accumulation** | 4 (effective batch = 8) |\n",
    "| **Epochs** | 1 |\n",
    "| **LoRA Rank** | 8 |\n",
    "| **Trainable Params** | ~4.2M (lower than rank 16) |\n",
    "| **Expected Train Loss** | ~1.82 |\n",
    "| **Expected Val Loss** | ~1.91 |\n",
    "\n",
    "### Key Observations\n",
    "\n",
    "- **Lower LoRA rank (r=8)** reduces trainable parameters and memory footprint\n",
    "- **Single epoch** provides faster training but may underfit compared to multi-epoch runs\n",
    "- **Higher learning rate (2e-4)** enables faster convergence in limited training time\n",
    "- Suitable for rapid prototyping and initial baseline establishment\n",
    "\n",
    "### Comparison to Other Experiments\n",
    "\n",
    "This experiment serves as a baseline for understanding the impact of training duration and model capacity (LoRA rank) on performance.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
